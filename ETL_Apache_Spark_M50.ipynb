{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e543091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3a3c98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create path for data file and load data\n",
    "path = \"traffic/per-vehicle-records-2021-01-15.csv\"\n",
    "data = spark.read.format(\"csv\").options(header=True).load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5f66c47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cosit', 'year', 'month', 'day', 'hour', 'minute', 'second', 'millisecond', 'minuteofday', 'lane', 'lanename', 'straddlelane', 'straddlelanename', 'class', 'classname', 'length', 'headway', 'gap', 'speed', 'weight', 'temperature', 'duration', 'validitycode', 'numberofaxles', 'axleweights', 'axlespacings']\n"
     ]
    }
   ],
   "source": [
    "# Get names of columns for future reference\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454f28df",
   "metadata": {},
   "source": [
    "## # 1. Calculate the usage of Irish road network in terms of percentage grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bff884a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---+\n",
      "|category|percentage| id|\n",
      "+--------+----------+---+\n",
      "|     CAR|     70.24|  0|\n",
      "| HGV_ART|      7.57|  1|\n",
      "|     BUS|      0.78|  2|\n",
      "| HGV_RIG|      4.37|  3|\n",
      "|    null|      0.01|  4|\n",
      "| CARAVAN|      0.62|  5|\n",
      "|     LGV|     15.84|  6|\n",
      "|   MBIKE|      0.56|  7|\n",
      "+--------+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Calculate the usage of Irish road network in terms of percentage grouped\n",
    "# by vehicle category.\n",
    "\n",
    "total = data.count()\n",
    "q1 = data.groupby('classname').count().withColumnRenamed('classname','category')\\\n",
    ".withColumn('percentage', f.round(f.col('count')/total*100 , 2)).drop('count')\\\n",
    ".withColumn('id', f.monotonically_increasing_id())\n",
    "q1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c29b69",
   "metadata": {},
   "source": [
    "## Move above data frame to Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29498d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move above data frame to Cassandra\n",
    "q1.select(\"id\", \"category\", \"percentage\")\\\n",
    ".write.format(\"org.apache.spark.sql.cassandra\")\\\n",
    ".options(table=\"percentage_per_vehicle_category\", keyspace=\"sensors\")\\\n",
    ".save(mode=\"append\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f512f0",
   "metadata": {},
   "source": [
    "## Read stored data frame from Cassndra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5442c8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+\n",
      "| id|category|percentage|\n",
      "+---+--------+----------+\n",
      "|  0|     CAR|     70.24|\n",
      "|  1| HGV_ART|      7.57|\n",
      "|  2|     BUS|      0.78|\n",
      "|  3| HGV_RIG|      4.37|\n",
      "|  4|    null|      0.01|\n",
      "|  5| CARAVAN|      0.62|\n",
      "|  6|     LGV|     15.84|\n",
      "|  7|   MBIKE|      0.56|\n",
      "+---+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read stored data frame from Cassndra\n",
    "spark.read.format(\"org.apache.spark.sql.cassandra\")\\\n",
    ".load(keyspace='traffic', table='percentage_per_vehicle_category').orderBy('id').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6b08b6",
   "metadata": {},
   "source": [
    "## These are locations for each sensor of motorway junctions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce094291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cosit for each junction for motorways\n",
    "# Note: Cosit for Jn01-Jn02 Dublin port to Santry is not available on the site map\n",
    "m50 = {'000000001012' : 'Jn02-Jn03',\n",
    "      '000000001500':'Jn03-Jn04',\n",
    "      '000000001501':'Jn04-Jn05',\n",
    "      '000000001502':'Jn05-Jn06',\n",
    "      '000000001508':'Jn06-Jn07',\n",
    "      '000000001503':'Jn07-Jn09',\n",
    "      '000000001509':'Jn09-Jn10',\n",
    "      '000000001504':'Jn10-Jn11',\n",
    "      '000000001505':'Jn11-Jn12',\n",
    "      '000000001506':'Jn12-Jn13',\n",
    "      '000000001507':'Jn13-Jn14',\n",
    "      '000000015010':'Jn14-Jn15',\n",
    "      '000000015011':'Jn15-Jn16',\n",
    "      '000000015012':'Jn16-Jn17'\n",
    "      }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaacb28",
   "metadata": {},
   "source": [
    "## # 2. Calculate the highest and lowest hourly flows on M50 - show the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22c9016a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+-------+\n",
      "|hour|vehicle_count|   flow|\n",
      "+----+-------------+-------+\n",
      "|  16|        38655|highest|\n",
      "|   2|         1167| lowest|\n",
      "+----+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Calculate the highest and lowest hourly flows on M50 - show the\n",
    "# hours and total number of vehicle counts\n",
    "\n",
    "hourly_flows = data.select(\"cosit\", \"hour\").where(f.col('cosit').isin(list(m50.keys()))).groupBy('hour').count().sort('count')\n",
    "mx = hourly_flows.agg({'count' : 'max'}).collect()[0][0] # Collect Max Value\n",
    "mn = hourly_flows.agg({'count' : 'min'}).collect()[0][0] # Collect Min value\n",
    "\n",
    "# Select row where max value\n",
    "mx_flow = hourly_flows.select('hour', 'count').where(f.col('count') == mx).withColumn('flow', f.lit('highest'))\n",
    "# Select row where min value\n",
    "mn_flow = hourly_flows.select('hour', 'count').where(f.col('count') == mn).withColumn('flow', f.lit('lowest'))\n",
    "\n",
    "# Union both max and min rows to form a table\n",
    "motorway_hourly_flows = mx_flow.union(mn_flow).withColumnRenamed('count', 'vehicle_count')\n",
    "motorway_hourly_flows.show() # add auto_increment column for ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a532df",
   "metadata": {},
   "source": [
    "## Move above data frame to Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1456884",
   "metadata": {},
   "outputs": [],
   "source": [
    "motorway_hourly_flows.select(\"flow\", \"hour\", \"vehicle_count\")\\\n",
    ".write.format(\"org.apache.spark.sql.cassandra\")\\\n",
    ".options(table=\"motorway_hourly_flows\", keyspace=\"sensors\")\\\n",
    ".save(mode=\"append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4649e327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-------------+\n",
      "|   flow|hour|vehicle_count|\n",
      "+-------+----+-------------+\n",
      "| lowest|   2|         1167|\n",
      "|highest|  16|        38655|\n",
      "+-------+----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"org.apache.spark.sql.cassandra\")\\\n",
    ".load(keyspace='traffic', table='motorway_hourly_flows').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30a8af5",
   "metadata": {},
   "source": [
    "## 3. Calculate the evening and morning rush hours on M50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbf35001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-------+\n",
      "|hour|count|   time|\n",
      "+----+-----+-------+\n",
      "|   7|22528|morning|\n",
      "|   8|27180|morning|\n",
      "|   6|18728|morning|\n",
      "|   9|29992|morning|\n",
      "|  10|29279|morning|\n",
      "|  16|38655|evening|\n",
      "|  18|18173|evening|\n",
      "|  17|36016|evening|\n",
      "|  19|13788|evening|\n",
      "|  20|11647|evening|\n",
      "+----+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Calculate the evening and morning rush hours on M50 - show the\n",
    "# hours and the total counts.\n",
    "\n",
    "morning_hours = ['6','7','8','9','10']\n",
    "evening_hours = ['16','17','18','19','20']\n",
    "\n",
    "data.head()\n",
    "morning_flows = data.select('hour', 'cosit').where(f.col('cosit').isin(list(m50.keys())))\\\n",
    ".where(f.col('hour').isin(morning_hours))\\\n",
    ".groupby('hour').count().withColumn('time', f.lit('morning'))\n",
    "\n",
    "evening_flows = data.select('hour', 'cosit').where(f.col('cosit').isin(list(m50.keys())))\\\n",
    ".where(f.col('hour').isin(evening_hours))\\\n",
    ".groupby('hour').count().withColumn('time', f.lit('evening'))\n",
    "\n",
    "rush_hours = morning_flows.union(evening_flows)\n",
    "\n",
    "rush_hours.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea598b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "rush_hours.select(\"hour\", \"count\", \"time\")\\\n",
    ".write.format(\"org.apache.spark.sql.cassandra\")\\\n",
    ".options(table=\"motorway_rush_hour\", keyspace=\"sensors\")\\\n",
    ".save(mode=\"append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61c910d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-------+\n",
      "|hour|count|   time|\n",
      "+----+-----+-------+\n",
      "|  18|18173|evening|\n",
      "|   9|29992|morning|\n",
      "|  17|36016|evening|\n",
      "|  20|11647|evening|\n",
      "|   7|22528|morning|\n",
      "|  10|29279|morning|\n",
      "|  16|38655|evening|\n",
      "|  19|13788|evening|\n",
      "|   8|27180|morning|\n",
      "|   6|18728|morning|\n",
      "+----+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"org.apache.spark.sql.cassandra\")\\\n",
    ".load(keyspace='sensors', table='motorway_rush_hour').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49321c24",
   "metadata": {},
   "source": [
    "## # 4. Calculate average speed between each junction on M50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b88ee8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|       cosit|           average|\n",
      "+------------+------------------+\n",
      "|000000001500| 88.83526554404145|\n",
      "|000000015011|104.02299711199059|\n",
      "|000000001505| 98.92545893412945|\n",
      "|000000001503| 98.45699912510936|\n",
      "|000000001509| 94.73736586836881|\n",
      "|000000001502| 99.01588546773877|\n",
      "|000000001507|102.64251095162643|\n",
      "|000000001506|102.11667798306114|\n",
      "|000000001501| 98.10988853617204|\n",
      "|000000001012| 84.09989342515166|\n",
      "|000000015010|106.05619648259243|\n",
      "|000000015012|106.45533712709087|\n",
      "|000000001504|100.41781593019984|\n",
      "|000000001508| 96.13615310118321|\n",
      "+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Calculate average speed between each junction on M50 (e.g., junction\n",
    "# 1 - junction2, junction 2 - junction 3, etc.).\n",
    "\n",
    "motorway_data = data.where(f.col('cosit').isin(list(m50.keys())))\n",
    "average_speed = motorway_data.select('cosit',col('speed').cast('double').alias('speed')).groupby('cosit').mean('speed')\n",
    "average_speed = average_speed.withColumnRenamed('avg(speed)', 'average')\n",
    "average_speed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0421f699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+\n",
      "|       cosit| location|\n",
      "+------------+---------+\n",
      "|000000001012|Jn02-Jn03|\n",
      "|000000001500|Jn03-Jn04|\n",
      "|000000001501|Jn04-Jn05|\n",
      "|000000001502|Jn05-Jn06|\n",
      "|000000001508|Jn06-Jn07|\n",
      "|000000001503|Jn07-Jn09|\n",
      "|000000001509|Jn09-Jn10|\n",
      "|000000001504|Jn10-Jn11|\n",
      "|000000001505|Jn11-Jn12|\n",
      "|000000001506|Jn12-Jn13|\n",
      "|000000001507|Jn13-Jn14|\n",
      "|000000015010|Jn14-Jn15|\n",
      "|000000015011|Jn15-Jn16|\n",
      "|000000015012|Jn16-Jn17|\n",
      "+------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "average_speed.createTempView('average_speed')\n",
    "location = spark.createDataFrame(data=m50.items(), schema=['cosit', 'location'])\n",
    "location.createTempView('location')\n",
    "location.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50b5773d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+-------------+\n",
      "|       cosit| location|average_speed|\n",
      "+------------+---------+-------------+\n",
      "|000000001012|Jn02-Jn03|         84.1|\n",
      "|000000001500|Jn03-Jn04|        88.84|\n",
      "|000000001501|Jn04-Jn05|        98.11|\n",
      "|000000001502|Jn05-Jn06|        99.02|\n",
      "|000000001508|Jn06-Jn07|        96.14|\n",
      "|000000001503|Jn07-Jn09|        98.46|\n",
      "|000000001509|Jn09-Jn10|        94.74|\n",
      "|000000001504|Jn10-Jn11|       100.42|\n",
      "|000000001505|Jn11-Jn12|        98.93|\n",
      "|000000001506|Jn12-Jn13|       102.12|\n",
      "|000000001507|Jn13-Jn14|       102.64|\n",
      "|000000015010|Jn14-Jn15|       106.06|\n",
      "|000000015011|Jn15-Jn16|       104.02|\n",
      "|000000015012|Jn16-Jn17|       106.46|\n",
      "+------------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "motorway_average_speed= spark.sql(\"SELECT average_speed.cosit, location, ROUND(average, 2) as average_speed \\\n",
    "          FROM average_speed, location\\\n",
    "          WHERE average_speed.cosit = location.cosit ORDER BY location\")\n",
    "motorway_average_speed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e63a83c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "motorway_average_speed.select(\"cosit\", \"location\", \"average_speed\")\\\n",
    ".write.format(\"org.apache.spark.sql.cassandra\")\\\n",
    ".options(table=\"motorway_average_speed\", keyspace=\"sensors\")\\\n",
    ".save(mode=\"append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "450a0c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+---------+\n",
      "|cosit|average_speed| location|\n",
      "+-----+-------------+---------+\n",
      "| 1507|       102.64|Jn13-Jn14|\n",
      "| 1508|        96.14|Jn06-Jn07|\n",
      "| 1505|        98.93|Jn11-Jn12|\n",
      "|15011|       104.02|Jn15-Jn16|\n",
      "| 1504|       100.42|Jn10-Jn11|\n",
      "| 1501|        98.11|Jn04-Jn05|\n",
      "|15010|       106.06|Jn14-Jn15|\n",
      "| 1509|        94.74|Jn09-Jn10|\n",
      "| 1012|         84.1|Jn02-Jn03|\n",
      "| 1502|        99.02|Jn05-Jn06|\n",
      "| 1506|       102.12|Jn12-Jn13|\n",
      "|15012|       106.46|Jn16-Jn17|\n",
      "| 1503|        98.46|Jn07-Jn09|\n",
      "| 1500|        88.84|Jn03-Jn04|\n",
      "+-----+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"org.apache.spark.sql.cassandra\")\\\n",
    ".load(keyspace='sensors', table='motorway_average_speed').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6003e16",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o438.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 57.0 failed 1 times, most recent failure: Lost task 0.0 in stage 57.0 (TID 63) (10.0.2.15 executor driver): java.io.FileNotFoundException: /tmp/blockmgr-16ee883d-85f4-42f6-b763-a25a6b4f14e3/23/temp_shuffle_0562a80d-e4bf-45e4-9871-6582ee49b5a2 (No space left on device)\n\tat java.io.FileOutputStream.open0(Native Method)\n\tat java.io.FileOutputStream.open(FileOutputStream.java:270)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:213)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:133)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:152)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:279)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.io.FileNotFoundException: /tmp/blockmgr-16ee883d-85f4-42f6-b763-a25a6b4f14e3/23/temp_shuffle_0562a80d-e4bf-45e4-9871-6582ee49b5a2 (No space left on device)\n\tat java.io.FileOutputStream.open0(Native Method)\n\tat java.io.FileOutputStream.open(FileOutputStream.java:270)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:213)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:133)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:152)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:279)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-a0f6e49cf55c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# (class). Map the COSITs with their names given on the map.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mhgv_locations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cosit\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'classname'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'HGV'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cosit'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'count'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mhgv_locations\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhgv_locations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mhgv_locations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Alice'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Bob'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m         \"\"\"\n\u001b[0;32m--> 744\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    691\u001b[0m         \"\"\"\n\u001b[1;32m    692\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o438.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 57.0 failed 1 times, most recent failure: Lost task 0.0 in stage 57.0 (TID 63) (10.0.2.15 executor driver): java.io.FileNotFoundException: /tmp/blockmgr-16ee883d-85f4-42f6-b763-a25a6b4f14e3/23/temp_shuffle_0562a80d-e4bf-45e4-9871-6582ee49b5a2 (No space left on device)\n\tat java.io.FileOutputStream.open0(Native Method)\n\tat java.io.FileOutputStream.open(FileOutputStream.java:270)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:213)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:133)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:152)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:279)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.io.FileNotFoundException: /tmp/blockmgr-16ee883d-85f4-42f6-b763-a25a6b4f14e3/23/temp_shuffle_0562a80d-e4bf-45e4-9871-6582ee49b5a2 (No space left on device)\n\tat java.io.FileOutputStream.open0(Native Method)\n\tat java.io.FileOutputStream.open(FileOutputStream.java:270)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:213)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:133)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:152)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:279)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "# 5. Calculate the top 10 locations with highest number of counts of HGVs\n",
    "# (class). Map the COSITs with their names given on the map.\n",
    "\n",
    "hgv_locations = data.select(\"cosit\").where(f.col('classname').contains('HGV')).groupby('cosit').count().sort(col('count').desc()).take(10)\n",
    "hgv_locations= spark.createDataFrame(hgv_locations)\n",
    "hgv_locations.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "69285214",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o595.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 64.0 failed 1 times, most recent failure: Lost task 0.0 in stage 64.0 (TID 76) (10.0.2.15 executor driver): java.io.FileNotFoundException: /tmp/blockmgr-16ee883d-85f4-42f6-b763-a25a6b4f14e3/1a/temp_shuffle_97348e9c-3f37-42d8-954d-2679cf0c428b (No space left on device)\n\tat java.io.FileOutputStream.open0(Native Method)\n\tat java.io.FileOutputStream.open(FileOutputStream.java:270)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:213)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:133)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:152)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:279)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.io.FileNotFoundException: /tmp/blockmgr-16ee883d-85f4-42f6-b763-a25a6b4f14e3/1a/temp_shuffle_97348e9c-3f37-42d8-954d-2679cf0c428b (No space left on device)\n\tat java.io.FileOutputStream.open0(Native Method)\n\tat java.io.FileOutputStream.open(FileOutputStream.java:270)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:213)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:133)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:152)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:279)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-4195752b29f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mhgv_locations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cosit\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'classname'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'HGV'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cosit'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'count'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhgv_locations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o595.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 64.0 failed 1 times, most recent failure: Lost task 0.0 in stage 64.0 (TID 76) (10.0.2.15 executor driver): java.io.FileNotFoundException: /tmp/blockmgr-16ee883d-85f4-42f6-b763-a25a6b4f14e3/1a/temp_shuffle_97348e9c-3f37-42d8-954d-2679cf0c428b (No space left on device)\n\tat java.io.FileOutputStream.open0(Native Method)\n\tat java.io.FileOutputStream.open(FileOutputStream.java:270)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:213)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:133)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:152)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:279)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.io.FileNotFoundException: /tmp/blockmgr-16ee883d-85f4-42f6-b763-a25a6b4f14e3/1a/temp_shuffle_97348e9c-3f37-42d8-954d-2679cf0c428b (No space left on device)\n\tat java.io.FileOutputStream.open0(Native Method)\n\tat java.io.FileOutputStream.open(FileOutputStream.java:270)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:213)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:133)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:152)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:279)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "hgv_locations = data.select(\"cosit\").where(f.col('classname').contains('HGV')).groupby('cosit').count().sort(col('count').desc()).limit(10)\n",
    "hgv_locations.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918e3c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_maps = {'000000001500':'Jn03-Jn04',s\n",
    "          '000000001501':'Jn04-Jn05',\n",
    "          '000000001503':'Jn07-Jn09',\n",
    "          '000000001502':'Jn05-Jn06',\n",
    "          '000000001508':'Jn06-Jn07',\n",
    "          '000000001070':'Jn01-Jn1a',\n",
    "          '000000001071':'Jn02-Jn03',\n",
    "          '000000001072':'Jn01a-Jn02',\n",
    "          '000000000997':'not_assigned',\n",
    "          '000000000998':'not_assigned2'\n",
    "                }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a6d8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_maps_df = spark.createDataFrame(data=location_maps.items(), schema=['cosit', 'location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77ca56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_maps_df.createTempView('location_maps_df')\n",
    "hgv_locations.createTempView('hgv_locations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e47f464",
   "metadata": {},
   "outputs": [],
   "source": [
    "q5 = spark.sql('SELECT hgv_locations.cosit, location_maps_df.location, count \\\n",
    "FROM location_maps_df, hgv_locations \\\n",
    "WHERE location_maps_df.cosit = hgv_locations.cosit')\n",
    "q5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b42f70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "q5.select(\"cosit\", \"location\", \"count\")\\\n",
    ".write.format(\"org.apache.spark.sql.cassandra\")\\\n",
    ".options(table=\"top_hgv_locations\", keyspace=\"traffic\")\\\n",
    ".save(mode=\"append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb4cf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format(\"org.apache.spark.sql.cassandra\")\\\n",
    ".load(keyspace='traffic', table='top_hgv_locations').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
